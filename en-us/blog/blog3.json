{
  "filename": "blog3.md",
  "__html": "<h1>UnitedDeploymemt - Supporting Multi-domain Workload Management</h1>\n<p>By FEI GUO (Alibaba), NOVEMBER 20 2019, 6 MINUTE READ</p>\n<p>Ironically, probably every cloud user knew (or should realized that) failures in Cloud resources\nare inevitable. Hence, high availability is probably one of the most desirable features that\nCloud Provider offers to cloud users. For example, in AWS, each geographic region has\nmultiple isolated locations known as Availability Zones (AZs).\nAWS provides various AZ-aware solutions to allow the compute or storage resources of the user\napplications to be distributed across multiple AZs in order to tolerate AZ failure, which indeed\nhappened in the past.</p>\n<p>In Kubernetes, the concept of AZ is not realized by an API object. Instead,\nan AZ is usually represented by a set of hosts that have the same location label.\nAlthough hosts within the same AZ can be identified by labels, the capability of distributing Pods across\nmultiple AZs was missing in Kubernetes default scheduler. Hence it was difficult to use single\n<code>StatefulSet</code> or <code>Deployment</code> to perform  AZ-aware workload management. Fortunately,\nin Kubernetes 1.16, a new feature called <a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\">&quot;Pod Topology Spread Constraints&quot;</a>\nwas invented. Users now can add new constraints in the Pod Spec to ensure the Pods of an\napplication can be evenly distributed across failure domains such as AZs, regions or nodes.</p>\n<p>In Kruise, <strong>UnitedDeploymemt</strong> provides an alternative to achieve high availability in\na cluster that spans over multiple fault domains - that is, managing multiple homogeneous\nworkloads each manage Pods within a single <code>Subset</code>. Pod distribution across AZs are\ndetermined by the replica number of each workload.\nSince each <code>Subset</code> is associated with a workload, UnitedDeployment can support\nfiner-grained rollout and deployment strategies.\nIn addition, it can be further extended to support managing\nan application across multiple clusters! Let us reveal how UnitedDeployment is designed.</p>\n<h2>Using <code>Subsets</code> to describe domain topology</h2>\n<p>UnitedDeploymemt uses <code>Subset</code> to represent a failure domain. <code>Subset</code> API\nprimarily specifies the nodes that forms the domain and the number of replicas run\nin this domain. UnitedDeploymemt manages subset workloads against a\nspecific domain topology, described by a <code>Subset</code> array.</p>\n<pre><code>type Topology struct {\n\t// Contains the details of each subset.\n\tSubsets []Subset\n}\n\n// Subset defines the detail of a subset.\ntype Subset struct {\n\t// Indicates the name of this subset, which will be used to generate\n\t// subset workload name in the format '&lt;uniteddeployment-name&gt;-&lt;subset-name&gt;'.\n\tName string\n\n\t// Indicates the node select strategy to form the subset.\n\tNodeSelector corev1.NodeSelector\n\n\t// Indicates the number of the subset replicas or percentage of it on the\n\t// UnitedDeployment replicas.\n\t// If nil, the number of replicas in this subset is determined by controller.\n\tReplicas *intstr.IntOrString\n}\n</code></pre>\n<p>The specification of the subset workload is specified in <code>Spec.Template</code>. UnitedDeployment\nonly supports <code>StatefulSet</code> subset workload as of now. An interesting use case of <code>Subset</code>\nis that now user can specify <strong>customized Pod distribution</strong> across AZs, which is not\nnecessarily a uniform distribution in some cases. For example, if the AZ\nutilization or capacity are not homogeneous, evenly distributing Pods may lead to Pod deployment\nfailure due to lack of resources. If users have prior knowledge about AZ resource capacity/usage,\nUnitedDeployment can help to apply an optimal Pod distribution for a high Pod deployment\nsuccess rate. Of course, if not specified, a uniform Pod distribution will be applied to\nmaximize availability.</p>\n<h2>Customized subset rollout <code>Partitions</code></h2>\n<p>User can update all the UnitedDeployment subset workloads by providing a\nnew version of subset workload template.\nSimilar to other Kruise controllers, UnitedDeployment controller does not orchestrate\nthe rollout process of all subset workloads, which is typically done by another rollout\ncontroller built on top of it. To help a rollout controller to realize flexible rollout plans\nlike canary or batch release, UnitedDeployment provides <code>ManualUpdate</code> strategy\nwhich allows user to specify the individual rollout <code>partition</code> of each subset workload.</p>\n<pre><code>type UnitedDeploymentUpdateStrategy struct {\n\t// Type of UnitedDeployment update.\n\tType UpdateStrategyType\n\t// Indicate the partition of each subset.\n\tManualUpdate *ManualUpdate\n}\n\n// ManualUpdate is a update strategy which allow users to provide the partition\n// of each subset.\ntype ManualUpdate struct {\n\t// Indicates number of subset partition.\n\tPartitions map[string]int32\n}\n</code></pre>\n<p>Now, it is fairly easy to implement subset-grained canary roll out for application\nwhose instances spread over multiple subsets.</p>\n<h2>Multi-Cluster application management (In future)</h2>\n<p>UnitedDeployment can potentially be extended to support multi-cluster workload\nmanagement with additional API support. The idea is that <code>Subset</code> can not only\nbe used to specify a domain within the cluster, but also be used to specify\na domain in another cluster. More specifically, a domain topology also includes\na <code>ClusterRegistryQuerySpec</code>, which describes the clusters that UnitedDeployment\nmay access. The cluster CRs are managed by a ClusterRegistry controller that\nimplements the Kubernetes <a href=\"https://github.com/kubernetes/cluster-registry\">cluster registry API</a>.</p>\n<pre><code>type Topology struct {\n  // ClusterRegistryQuerySpec is used to find the all the clusters that\n  // the workload may be deployed to. \n  ClusterRegistry *ClusterRegistryQuerySpec\n  // Contains the details of each subset including the target cluster name and\n  // the node selector in target cluster.\n  Subsets []Subset\n}\n\ntype ClusterRegistryQuerySpec struct {\n  // Namespaces that the cluster CRDs reside.\n  // If not specified, default namespace is used.\n  Namespaces []string\n  // Selector is the label matcher to find all qualified clusters.\n  Selector   map[string]string\n  // Describe the kind and APIversion of the cluster object\n  ClusterType metav1.TypeMeta\n}\n\ntype Subset struct {\n  // Indicate the name of this subset, which will be used to generate \n  // subset workload name in the format '&lt;deployment-name&gt;-&lt;subset-name&gt;'\n  Name string\n    \n  // The name of target cluster. The controller will validate that\n  // the TargetCluster exits based on Topology.ClusterRegistry.\n  TargetCluster *TargetCluster\n\n  // Indicate the node select strategy in the Subset.TargetCluster.\n  // If Subset.TargetCluster is not set, node selector strategy refers to\n  // current cluster. \n  NodeSelector corev1.NodeSelector\n\n  // Indicate the number of pod replicas of this subset.\n  // If nil, the number of replicas in this subset is determined by controller.\n  Replicas *intstr.IntOrString \n}\n\ntype TargetCluster struct {\n  // Namespace of the target cluster CRD\n  Namespace string\n  // Target cluster name\n  Name string\n}\n</code></pre>\n<p>A new <code>TargetCluster</code> field is added to the <code>Subset</code> API. If it presents, the\n<code>NodeSelector</code> indicates the node selection logic in the target cluster. Now\nUnitedDeployment controller can distribute application Pods to multiple clusters by\ninstantiating a <code>StatefulSet</code> workload in each target cluster with a specific\nreplica number, as illustrated in Figure1.</p>\n<p><img src=\"/img/uniteddeployment.png\" alt=\"multi-cluster\tcontroller\"></p>\n<p>At a first glance, UnitedDeployment looks more like a federation\ncontroller under <a href=\"https://github.com/kubernetes-sigs/kubefed\">Kubefed</a>, but it isn't.\nThe fundamental difference is that Kubefed focuses on propagating arbitrary\ntypes to remote clusters instead of managing an application across clusters.\nIn this example, had Kubefed been used, each <code>StatefulSet</code> workload in individual\ncluster would have a replica of 100. With customized <code>Partition</code> support as described\nabove, cluster-grained application canary roll can be easily achieved by a rollout\ncontroller.</p>\n<h2>Summary</h2>\n<p>This blog post introduces UnitedDeployment, a new workload which helps managing\napplication spread over multiple domains (in arbitrary clusters). By reading this post,\nI wish readers would understand that UnitedDeployment does more than just evenly\ndistributing Pods over AZs, which can be done more efficiently by using the new Pod\nTopology Spread Constraint APIs in Kubernetes 1.16 if it is the only user concern.</p>\n",
  "link": "/en-us/blog/blog3.html",
  "meta": {
    "title": "UnitedDeploymemt - Supporting Multi-domain Workload Management",
    "keywords": "Kubernetes, controller",
    "description": ""
  }
}